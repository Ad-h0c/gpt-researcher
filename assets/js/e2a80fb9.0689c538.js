"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[304],{3905:(e,n,t)=>{t.d(n,{Zo:()=>u,kt:()=>d});var a=t(7294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function i(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var p=a.createContext({}),s=function(e){var n=a.useContext(p),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},u=function(e){var n=s(e.components);return a.createElement(p.Provider,{value:n},e.children)},c={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},m=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,o=e.originalType,p=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),m=s(t),d=r,h=m["".concat(p,".").concat(d)]||m[d]||c[d]||o;return t?a.createElement(h,l(l({ref:n},u),{},{components:t})):a.createElement(h,l({ref:n},u))}));function d(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=t.length,l=new Array(o);l[0]=m;var i={};for(var p in n)hasOwnProperty.call(n,p)&&(i[p]=n[p]);i.originalType=e,i.mdxType="string"==typeof e?e:r,l[1]=i;for(var s=2;s<o;s++)l[s]=t[s];return a.createElement.apply(null,l)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},9598:(e,n,t)=>{t.r(n),t.d(n,{contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>p});var a=t(7462),r=(t(7294),t(3905));const o={},l="Configure LLM",i={unversionedId:"gpt-researcher/llms",id:"gpt-researcher/llms",isDocsHomePage:!1,title:"Configure LLM",description:"As described in the introduction, the default LLM is OpenAI due to its superior performance and speed.",source:"@site/docs/gpt-researcher/llms.md",sourceDirName:"gpt-researcher",slug:"/gpt-researcher/llms",permalink:"/docs/gpt-researcher/llms",editUrl:"https://github.com/assafelovic/gpt-researcher/tree/master/docs/docs/gpt-researcher/llms.md",tags:[],version:"current",frontMatter:{},sidebar:"docsSidebar",previous:{title:"Retrievers",permalink:"/docs/gpt-researcher/retrievers"},next:{title:"LangGraph",permalink:"/docs/gpt-researcher/langgraph"}},p=[{value:"Custom OpenAI",id:"custom-openai",children:[{value:"Custom OpenAI API LLM",id:"custom-openai-api-llm",children:[],level:3},{value:"Custom OpenAI API Embedding",id:"custom-openai-api-embedding",children:[],level:3},{value:"Azure OpenAI",id:"azure-openai",children:[],level:3}],level:2},{value:"Ollama",id:"ollama",children:[],level:2},{value:"Groq",id:"groq",children:[{value:"Sign up",id:"sign-up",children:[],level:3},{value:"Update env vars",id:"update-env-vars",children:[],level:3}],level:2},{value:"Anthropic",id:"anthropic",children:[],level:2},{value:"Mistral",id:"mistral",children:[],level:2},{value:"Together AI",id:"together-ai",children:[],level:2},{value:"HuggingFace",id:"huggingface",children:[],level:2},{value:"Google Gemini",id:"google-gemini",children:[],level:2}],s={toc:p};function u(e){let{components:n,...t}=e;return(0,r.kt)("wrapper",(0,a.Z)({},s,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"configure-llm"},"Configure LLM"),(0,r.kt)("p",null,"As described in the ",(0,r.kt)("a",{parentName:"p",href:"/docs/gpt-researcher/config"},"introduction"),", the default LLM is OpenAI due to its superior performance and speed.\nWith that said, GPT Researcher supports various open/closed source LLMs, and you can easily switch between them by adding the ",(0,r.kt)("inlineCode",{parentName:"p"},"LLM_PROVIDER")," env variable and corresponding configuration params.\nCurrent supported LLMs are ",(0,r.kt)("inlineCode",{parentName:"p"},"openai"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"google")," (gemini), ",(0,r.kt)("inlineCode",{parentName:"p"},"azureopenai"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"ollama"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"anthropic"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"mistral"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"huggingface")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"groq"),"."),(0,r.kt)("p",null,"Using any model will require at least updating the ",(0,r.kt)("inlineCode",{parentName:"p"},"LLM_PROVIDER")," param and passing the LLM provider API Key. You might also need to update the ",(0,r.kt)("inlineCode",{parentName:"p"},"SMART_LLM_MODEL")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"FAST_LLM_MODEL")," env vars.\nTo learn more about support customization options see ",(0,r.kt)("a",{parentName:"p",href:"/gpt-researcher/config"},"here"),"."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Please note"),": GPT Researcher is optimized and heavily tested on GPT models. Some other models might run intro context limit errors, and unexpected responses.\nPlease provide any feedback in our ",(0,r.kt)("a",{parentName:"p",href:"https://discord.gg/DUmbTebB"},"Discord community")," channel, so we can better improve the experience and performance."),(0,r.kt)("p",null,"Below you can find examples for how to configure the various supported LLMs."),(0,r.kt)("h2",{id:"custom-openai"},"Custom OpenAI"),(0,r.kt)("p",null,"Create a local OpenAI API using ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md#quick-start"},"llama.cpp Server"),"."),(0,r.kt)("h3",{id:"custom-openai-api-llm"},"Custom OpenAI API LLM"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'# use a custom OpenAI API LLM provider\nLLM_PROVIDER="openai"\n\n# set the custom OpenAI API url\nOPENAI_BASE_URL="http://localhost:1234/v1"\n# set the custom OpenAI API key\nOPENAI_API_KEY="Your Key"\n\n# specify the custom OpenAI API llm model  \nFAST_LLM_MODEL="gpt-4o-mini"\n# specify the custom OpenAI API llm model  \nSMART_LLM_MODEL="gpt-4o"\n\n')),(0,r.kt)("h3",{id:"custom-openai-api-embedding"},"Custom OpenAI API Embedding"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'# use a custom OpenAI API EMBEDDING provider\nEMBEDDING_PROVIDER="custom"\n\n# set the custom OpenAI API url\nOPENAI_BASE_URL="http://localhost:1234/v1"\n# set the custom OpenAI API key\nOPENAI_API_KEY="Your Key"\n\n# specify the custom OpenAI API embedding model   \nOPENAI_EMBEDDING_MODEL="custom_model"\n')),(0,r.kt)("h3",{id:"azure-openai"},"Azure OpenAI"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'EMBEDDING_PROVIDER="azureopenai"\nAZURE_OPENAI_API_KEY="Your key"\n')),(0,r.kt)("h2",{id:"ollama"},"Ollama"),(0,r.kt)("p",null,"GPT Researcher supports both Ollama LLMs and embeddings. You can choose each or both.\nTo use ",(0,r.kt)("a",{parentName:"p",href:"http://www.ollama.com"},"Ollama")," you can set the following environment variables"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# Use ollama for both, LLM and EMBEDDING provider\nLLM_PROVIDER=ollama\n\n# Ollama endpoint to use\nOLLAMA_BASE_URL=http://localhost:11434\n\n# Specify one of the LLM models supported by Ollama\nFAST_LLM_MODEL=llama3\n# Specify one of the LLM models supported by Ollama \nSMART_LLM_MODEL=llama3 \n# The temperature to use, defaults to 0.55\nTEMPERATURE=0.55\n")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Optional")," - You can also use ollama for embeddings"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"EMBEDDING_PROVIDER=ollama\n# Specify one of the embedding models supported by Ollama \nOLLAMA_EMBEDDING_MODEL=nomic-embed-text\n")),(0,r.kt)("h2",{id:"groq"},"Groq"),(0,r.kt)("p",null,"GroqCloud provides advanced AI hardware and software solutions designed to deliver amazingly fast AI inference performance.\nTo leverage Groq in GPT-Researcher, you will need a GroqCloud account and an API Key. (",(0,r.kt)("strong",{parentName:"p"},"NOTE:")," Groq has a very ",(0,r.kt)("em",{parentName:"p"},"generous free tier"),".)"),(0,r.kt)("h3",{id:"sign-up"},"Sign up"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"You can signup here: ",(0,r.kt)("a",{parentName:"p",href:"https://console.groq.com/login"},"https://console.groq.com/login"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Once you are logged in, you can get an API Key here: ",(0,r.kt)("a",{parentName:"p",href:"https://console.groq.com/keys"},"https://console.groq.com/keys"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Once you have an API key, you will need to add it to your ",(0,r.kt)("inlineCode",{parentName:"p"},"systems environment")," using the variable name:\n",(0,r.kt)("inlineCode",{parentName:"p"},'GROQ_API_KEY="*********************"')))),(0,r.kt)("h3",{id:"update-env-vars"},"Update env vars"),(0,r.kt)("p",null,"And finally, you will need to configure the GPT-Researcher Provider and Model variables:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# To use Groq set the llm provider to groq\nLLM_PROVIDER=groq\nGROQ_API_KEY=[Your Key]\n\n# Set one of the LLM models supported by Groq\nFAST_LLM_MODEL=Mixtral-8x7b-32768\n\n# Set one of the LLM models supported by Groq\nSMART_LLM_MODEL=Mixtral-8x7b-32768 \n\n# The temperature to use defaults to 0.55\nTEMPERATURE=0.55\n")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"NOTE:")," As of the writing of this Doc (May 2024), the available Language Models from Groq are:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Llama3-70b-8192"),(0,r.kt)("li",{parentName:"ul"},"Llama3-8b-8192"),(0,r.kt)("li",{parentName:"ul"},"Mixtral-8x7b-32768"),(0,r.kt)("li",{parentName:"ul"},"Gemma-7b-it")),(0,r.kt)("h2",{id:"anthropic"},"Anthropic"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://www.anthropic.com/"},"Anthropic")," is an AI safety and research company, and is the creator of Claude. This page covers all integrations between Anthropic models and LangChain."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"LLM_PROVIDER=anthropic\nANTHROPIC_API_KEY=[Your key]\n")),(0,r.kt)("p",null,"You can then define the fast and smart LLM models for example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"FAST_LLM_MODEL=claude-2.1\nSMART_LLM_MODEL=claude-3-opus-20240229\n")),(0,r.kt)("p",null,"You can then define the fast and smart LLM models for example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"FAST_LLM_MODEL=claude-2.1\nSMART_LLM_MODEL=claude-3-opus-20240229\n")),(0,r.kt)("h2",{id:"mistral"},"Mistral"),(0,r.kt)("p",null,"Sign up for a ",(0,r.kt)("a",{parentName:"p",href:"https://console.mistral.ai/users/api-keys/"},"Mistral API key"),".\nThen update the corresponding env vars, for example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"LLM_PROVIDER=mistral\nANTHROPIC_API_KEY=[Your key]\nFAST_LLM_MODEL=open-mistral-7b\nSMART_LLM_MODEL=mistral-large-latest\n")),(0,r.kt)("h2",{id:"together-ai"},"Together AI"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://www.together.ai/"},"Together AI")," offers an API to query ",(0,r.kt)("a",{parentName:"p",href:"https://docs.together.ai/docs/inference-models"},"50+ leading open-source models")," in a couple lines of code.\nThen update corresponding env vars, for example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"LLM_PROVIDER=together\nTOGETHER_API_KEY=[Your key]\nFAST_LLM_MODEL=meta-llama/Llama-3-8b-chat-hf\nSMART_LLM_MODEL=meta-llama/Llama-3-70b-chat-hf\n")),(0,r.kt)("h2",{id:"huggingface"},"HuggingFace"),(0,r.kt)("p",null,"This integration requires a bit of extra work. Follow ",(0,r.kt)("a",{parentName:"p",href:"https://python.langchain.com/v0.1/docs/integrations/chat/huggingface/"},"this guide")," to learn more.\nAfter you've followed the tutorial above, update the env vars:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"LLM_PROVIDER=huggingface\nHUGGINGFACE_API_KEY=[Your key]\nFAST_LLM_MODEL=HuggingFaceH4/zephyr-7b-beta\nSMART_LLM_MODEL=HuggingFaceH4/zephyr-7b-beta\n")),(0,r.kt)("h2",{id:"google-gemini"},"Google Gemini"),(0,r.kt)("p",null,"Sign up ",(0,r.kt)("a",{parentName:"p",href:"https://ai.google.dev/gemini-api/docs/api-key"},"here")," for obtaining a Google Gemini API Key and update the following env vars:"),(0,r.kt)("p",null,"Please make sure to update fast and smart models to corresponding valid Gemini models."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"LLM_PROVIDER=google\nGEMINI_API_KEY=[Your key]\n")))}u.isMDXComponent=!0}}]);